{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Current OS:  win32\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# 사용자 설정\n",
    "#```\n",
    "USER = 'sjh'\n",
    "# USER = 'hse'\n",
    "\n",
    "# basic package\n",
    "import os, re, time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sys import platform\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# for crawling\n",
    "from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "chrome_opt = webdriver.ChromeOptions()\n",
    "\n",
    "# to unable image\n",
    "prefs = {\"profile.managed_default_content_settings.images\": 2}\n",
    "chrome_opt.add_experimental_option(\"prefs\", prefs) \n",
    "\n",
    "if USER == 'sjh':\n",
    "    print('>> Current OS: ', platform)\n",
    "    if 'darwin' in platform:\n",
    "        driverPATH = '/Users/huni/Dropbox/내 Mac (MacBook-Pro.local)/Downloads/chromedriver'\n",
    "        chrome_opt.add_argument('--kiosk')\n",
    "    else:\n",
    "        driverPATH = r'C:\\Users\\jhun1\\Dropbox\\My PC (LAPTOP-VLNR6K8R)\\Downloads\\chromedriver_win32\\chromedriver'\n",
    "        chrome_opt.add_experimental_option(\"excludeSwitches\", [\"enable-logging\"])\n",
    "        chrome_opt.add_argument('--start-maximized') # 창 최대\n",
    "        \n",
    "elif USER == 'hse':\n",
    "    driverPATH = \"C:\\Download\\chromedriver.exe\"\n",
    "    \n",
    "\n",
    "class Crawler:\n",
    "    def __init__(self, USER, seleniumPath, chromeOption):\n",
    "        self.user = USER\n",
    "        self.seleniumPath = seleniumPath\n",
    "        self.chromeOption = chromeOption\n",
    "        self.driver = webdriver.Chrome(executable_path=seleniumPath, chrome_options=chromeOption)\n",
    "        self.acc = ActionChains(self.driver)\n",
    "    \n",
    "    @staticmethod\n",
    "    #데이터 정리 함수\n",
    "    def get_str(str_tmp):\n",
    "        str_tmp = str_tmp.replace(\"\\n\", \" \")\n",
    "        str_tmp = str_tmp.replace(\"\\t\", \" \")\n",
    "        str_tmp = str_tmp.replace(\"                \", \" \")\n",
    "        str_tmp = str_tmp.strip()\n",
    "        return str_tmp\n",
    "\n",
    "\n",
    "    # 댓글이던, 영상이던 최하단까지 스크롤해줌\n",
    "    def scroll(self, scroll_time, wait_time, testOpt = False):\n",
    "        last_page_height = self.driver.execute_script(\n",
    "            \"return document.documentElement.scrollHeight\"\n",
    "        )\n",
    "\n",
    "        stand_height = 0\n",
    "        sub_height = last_page_height\n",
    "\n",
    "        #!# 스크롤을 불필요하게 길게함\n",
    "        start = time.time()\n",
    "        while True:\n",
    "            current_height = self.driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
    "\n",
    "            for i in range(10):\n",
    "                self.driver.execute_script(f\"window.scrollTo(0, {stand_height + (sub_height/10 * i)});\")\n",
    "                time.sleep(scroll_time)\n",
    "            time.sleep(wait_time)\n",
    "\n",
    "            new_page_height = self.driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
    "            stand_height = last_page_height\n",
    "            sub_height = new_page_height - last_page_height\n",
    "\n",
    "            if new_page_height == last_page_height:\n",
    "                break\n",
    "\n",
    "            endTime = time.time()\n",
    "            howLong = endTime - start\n",
    "            if testOpt == True and (howLong > 120): # 테스트모드 일 때는 1분만 스크롤함\n",
    "                 break\n",
    "            last_page_height = new_page_height\n",
    "            time.sleep(wait_time)\n",
    "\n",
    "\n",
    "    # 유튜브 들어가서 검색어 넣고 크롤링함\n",
    "    def oneSearch(self, searchWord, testOpt=False):\n",
    "        # 멀티프로세싱하려고 일부러 드라이버를 따로 부르는 시도 -> window, ipynb, interactive cell is impossible to multiprocessing\n",
    "        # driver = webdriver.Chrome(executable_path=self.seleniumPath, chrome_options=self.chromeOption)\n",
    "\n",
    "        output = pd.DataFrame(columns='영상명 채널명 조회수 URL 댓글수 reply'.split())\n",
    "        start = time.time()\n",
    "        self.driver.get(f'https://www.youtube.com/results?search_query={searchWord}')\n",
    "        time.sleep(2) # 충분히 element 로드\n",
    "\n",
    "        \n",
    "        Crawler.scroll(self, 2, 0.5, testOpt=testOpt)\n",
    "        \n",
    "        # 스크롤을 하드하게 하면 몇 개의 박스가 나올지 모름\n",
    "        renderBoxLS = self.driver.find_elements(By.CLASS_NAME, 'style-scope ytd-section-list-renderer')\n",
    "        for render in renderBoxLS:\n",
    "            container = self.driver.find_element(By.ID, 'contents')\n",
    "            videoLS = container.find_elements(By.XPATH, '//*[@id=\"meta\"]')\n",
    "\n",
    "        for idx, video in enumerate(videoLS):\n",
    "            try:\n",
    "                title = video.find_element(By.CSS_SELECTOR, '#video-title > yt-formatted-string').text\n",
    "                view = video.find_element(By.CSS_SELECTOR, '#metadata-line > span:nth-child(1)').text\n",
    "                date = video.find_element(By.CSS_SELECTOR, '#metadata-line > span:nth-child(2)').text\n",
    "                href = video.find_element(By.CSS_SELECTOR, '#video-title').get_attribute('href')\n",
    "\n",
    "                output.loc[idx, '영상명'] = title\n",
    "                output.loc[idx, '조회수'] = view\n",
    "                output.loc[idx, 'URL'] = href\n",
    "   \n",
    "            except NoSuchElementException:\n",
    "                break\n",
    "\n",
    "        print('>> function performance(sec): ', time.time()-start)\n",
    "        return output\n",
    "\n",
    "    def D1(self, searchWordLS):\n",
    "        output = pd.DataFrame(columns='영상명 채널명 조회수 URL 댓글수 reply'.split())\n",
    "\n",
    "        for word in searchWordLS:\n",
    "            print('\\n>> Word ', word, ' crawling start')\n",
    "            temp = Crawler.oneSearch(self, word, testOpt = False)\n",
    "            output = output.append(temp, ignore_index=True)\n",
    "            print(f'>> {word} crawled.', temp.shape)\n",
    "\n",
    "        output.reset_index(drop=True, inplace=True)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawler = Crawler(USER, driverPATH, chrome_opt)\n",
    "D1 = crawler.D1(['재중동포', '조선족'])\n",
    "# D1 = crawler.D1(['재중동포'])\n",
    "# finalRS = crawler.D2(D1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'D1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jhun1\\Proj\\data_preprocessing\\[crawler]youtube\\crawler.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/jhun1/Proj/data_preprocessing/%5Bcrawler%5Dyoutube/crawler.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m D1\u001b[39m.\u001b[39mto_excel(\u001b[39m'\u001b[39m\u001b[39mresult(220808).xlsx\u001b[39m\u001b[39m'\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'D1' is not defined"
     ]
    }
   ],
   "source": [
    "D1.to_excel('result(220808).xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClientRule:\n",
    "    def __init__(self, crawledDf):\n",
    "        if type(crawledDf) == pd.DataFrame:\n",
    "            self.df = crawledDf\n",
    "        elif type(crawledDf) == str:\n",
    "            self.df = pd.read_excel(os.path.join(os.getcwd(), crawledDf))\n",
    "\n",
    "    # 조회수에 '878 views' '212k views' 이런거 다 고쳐야함\n",
    "    @staticmethod\n",
    "    def viewCleaner(inputView):\n",
    "        delete = len('views') + 1 # 공백까지 카운트\n",
    "        txt = inputView[:-delete]\n",
    "        if re.search('[.KM]', txt): # . K M이 있나 확인\n",
    "            if re.search('K', txt):\n",
    "                kIdx = txt.find('K') \n",
    "                txt2 = txt[:kIdx]\n",
    "                output = float(txt2) * 1000\n",
    "            elif re.search('M', txt):\n",
    "                mIdx = txt.find('M')\n",
    "                txt2 = txt[:mIdx]\n",
    "                output = float(txt2) * 1000000\n",
    "\n",
    "        else: # 숫자만 있다는 것임\n",
    "            try:\n",
    "                output = float(txt)\n",
    "            except ValueError:\n",
    "                output = 0.0\n",
    "\n",
    "        return output\n",
    "    \n",
    "    @staticmethod\n",
    "    def duplicate(inputDf):\n",
    "        output = inputDf.drop_duplicates(subset=['URL'], keep='first', ignore_index=True)\n",
    "        return output\n",
    "\n",
    "    def cleaning(self, topN = 100):\n",
    "        ToDrop = []\n",
    "        for idx, val in enumerate(self.df['영상명'].values):\n",
    "            if ('재중동포' not in val) and ('조선족' not in val):\n",
    "                ToDrop.append(idx)\n",
    "        \n",
    "        before = self.df.shape[0]\n",
    "        sliceDf = self.df.drop(ToDrop, axis=0, inplace=True)\n",
    "        after =  self.df.shape[0]\n",
    "        print('>> 제목으로 정렬해서 탈락함: ', before - after)\n",
    "\n",
    "        self.df['조회수'] = self.df['조회수'].apply(lambda x: ClientRule.viewCleaner(x))\n",
    "        self.df = ClientRule.duplicate(self.df)\n",
    "    \n",
    "\n",
    "        sortedDf = self.df.sort_values(by=['조회수'])\n",
    "\n",
    "        sliceDf = sortedDf.iloc[-topN:, :]\n",
    "        sliceDf.reset_index(drop=True, inplace=True)\n",
    "        display(sliceDf.head())\n",
    "        return sliceDf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> 제목으로 정렬해서 탈락함:  184\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>영상명</th>\n",
       "      <th>채널명</th>\n",
       "      <th>조회수</th>\n",
       "      <th>URL</th>\n",
       "      <th>댓글수</th>\n",
       "      <th>reply</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>중국은 지금 '아리랑' 열풍 중...중국동포(조선족) 귀여운 가수 김시연의 노래! ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5400.0</td>\n",
       "      <td>https://www.youtube.com/watch?v=RsZ0Y6xzVEg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>신기에 가까운 \"물동이 춤\" 연길시조선족무용단</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5500.0</td>\n",
       "      <td>https://www.youtube.com/watch?v=bL1av4OGvtU</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>충격 조선족 오열 #shorts</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5500.0</td>\n",
       "      <td>https://www.youtube.com/shorts/QvhUypqlhFI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>한송이씨는 우주로 가라 연변조선족자치주 만세 세계적 웃음거리로다</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5500.0</td>\n",
       "      <td>https://www.youtube.com/shorts/e9zjH8sUc4E</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>동계올림픽에 조선족분이 한복 입은것에 대해... 라이브중 질문대답!</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5600.0</td>\n",
       "      <td>https://www.youtube.com/watch?v=nxCFVuiJIWw</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 영상명  채널명     조회수  \\\n",
       "0  중국은 지금 '아리랑' 열풍 중...중국동포(조선족) 귀여운 가수 김시연의 노래! ...  NaN  5400.0   \n",
       "1                          신기에 가까운 \"물동이 춤\" 연길시조선족무용단  NaN  5500.0   \n",
       "2                                  충격 조선족 오열 #shorts  NaN  5500.0   \n",
       "3                한송이씨는 우주로 가라 연변조선족자치주 만세 세계적 웃음거리로다  NaN  5500.0   \n",
       "4              동계올림픽에 조선족분이 한복 입은것에 대해... 라이브중 질문대답!  NaN  5600.0   \n",
       "\n",
       "                                           URL  댓글수  reply  \n",
       "0  https://www.youtube.com/watch?v=RsZ0Y6xzVEg  NaN    NaN  \n",
       "1  https://www.youtube.com/watch?v=bL1av4OGvtU  NaN    NaN  \n",
       "2   https://www.youtube.com/shorts/QvhUypqlhFI  NaN    NaN  \n",
       "3   https://www.youtube.com/shorts/e9zjH8sUc4E  NaN    NaN  \n",
       "4  https://www.youtube.com/watch?v=nxCFVuiJIWw  NaN    NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cleaner = ClientRule('result(220808).xlsx')\n",
    "del cleaner.df['Unnamed: 0']\n",
    "D1_RS = cleaner.cleaning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "\n",
    "\n",
    "class NextCrawler(Crawler):\n",
    "    def __init(self, USER, seleniumPath, chromeOption):\n",
    "        super().__init__(USER, seleniumPath, chromeOption)    \n",
    "        self.driver = webdriver.Chrome(executable_path=seleniumPath, chrome_options=chromeOption)\n",
    "        self.acc = ActionChains(self.driver)\n",
    "        \n",
    "\n",
    "        # 댓글이던, 영상이던 최하단까지 스크롤해줌\n",
    "    def scroll(self, waitTime, currentURL, testOpt = False):\n",
    "        self.acc.send_keys(Keys.END).perform() # 처음에는 한번 내려줌\n",
    "        fullLoaded = 0\n",
    "        totalScrollCount = 0\n",
    "        \n",
    "        try:\n",
    "            commentExist = self.driver.find_element(By.XPATH, '/html/body/ytd-app/div[1]/ytd-page-manager/ytd-watch-flexy/div[5]/div[1]/div/div[2]/ytd-comments/ytd-item-section-renderer/div[3]/ytd-message-renderer/yt-formatted-string[1]/span')\n",
    "            if commentExist.text == 'Comments are turned off.':\n",
    "                print('>> comments are turned off: ', currentURL)\n",
    "                return []\n",
    "        \n",
    "        except NoSuchElementException:\n",
    "            while fullLoaded < 3: # 댓글 수 변화가 없을 때까지 로드함\n",
    "                replyBox = self.driver.find_element(By.XPATH, '//*[@id=\"contents\"]') # 댓글 박스 전체\n",
    "                \n",
    "                replyCount_before = replyBox.find_elements(By.XPATH, '//*[@id=\"body\"]') # 각 댓글\n",
    "\n",
    "\n",
    "                self.acc.send_keys(Keys.PAGE_DOWN).perform()  # 키보드 END 키를 눌러서 동영상 더 로드하기 -> 절대 end를 누르면 안되고 page_down을 눌러야함\n",
    "                totalScrollCount += 1 # 무턱대고 오래 스크롤할 때가 있어서, 이렇게 에러코드 넣어줌\n",
    "                if totalScrollCount > 100:\n",
    "                    break\n",
    "                time.sleep(waitTime)\n",
    "\n",
    "                replyCount_after = replyBox.find_elements(By.XPATH, '//*[@id=\"body\"]')\n",
    "\n",
    "                if len(replyCount_before) == len(replyCount_after):\n",
    "                    fullLoaded += 1\n",
    "                else:\n",
    "                    fullLoaded = 0 # 이걸 해줘야, 데이터 로드에 잠깐 시간이 걸려도 불필요하게 카운트되지 않음\n",
    "            if testOpt == True:\n",
    "                print('>> all scrolled', len(replyCount_after))\n",
    "            return replyCount_after\n",
    "    \n",
    "    # @staticmethod\n",
    "    # def shortsReplcae(inputDf):\n",
    "    #     LS = list(inputDf['URL'].values)\n",
    "    #     newLS = []\n",
    "        \n",
    "    #     for val in LS:\n",
    "    #         if 'short' in val:\n",
    "    #             newVal = val.replace('shorts', 'watch?v=')\n",
    "    #             newLS.append(newVal)\n",
    "    #         else:\n",
    "    #             newLS.append(newVal)\n",
    "\n",
    "    #     inputDf['URL'] = newLS\n",
    "    #     return inputDf\n",
    "        \n",
    "    \n",
    "    # 한 영상의 댓글을 긁어옴\n",
    "    def earnReply(self, href):\n",
    "        output = pd.DataFrame(columns='영상명 채널명 조회수 URL reply'.split())\n",
    "        self.driver.get(href)\n",
    "        wait = WebDriverWait(self.driver, 10)\n",
    "        element = wait.until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"contents\"]')))\n",
    "        \n",
    "\n",
    "        replyCount_after = NextCrawler.scroll(self, 1.5, href) # 1초하니까 댓글이 있는데도 안긁히는 영상이 생김\n",
    "        if len(replyCount_after) == 0 :\n",
    "            print('>> no reply: ', href)\n",
    "\n",
    "        if self.user == 'hse':\n",
    "            html_source = self.driver.page_source\n",
    "            soup = BeautifulSoup(html_source, 'lxml')\n",
    "\n",
    "            youtube_user_IDs = soup.select('div#header-author > a > span')\n",
    "            youtube_comments = soup.select('yt-formatted-string#content-text')\n",
    "\n",
    "            str_comments = []\n",
    "            for i in range(len(youtube_comments)):\n",
    "                str_comments.append(Crawler.get_str(str(youtube_comments[i].text)))\n",
    "        \n",
    "        elif self.user == 'sjh':\n",
    "            str_comments = []\n",
    "\n",
    "            for oneReply in replyCount_after:\n",
    "                reply = oneReply.find_element(By.CSS_SELECTOR, '#content-text').text\n",
    "                str_comments.append(reply)\n",
    "        \n",
    "        title = self.driver.find_element(By.XPATH, '/html/body/ytd-app/div[1]/ytd-page-manager/ytd-watch-flexy/div[5]/div[1]/div/div[2]/div[5]/div[1]/div[2]/ytd-video-primary-info-renderer/div/h1/yt-formatted-string').text\n",
    "        view = self.driver.find_element(By.XPATH, '/html/body/ytd-app/div[1]/ytd-page-manager/ytd-watch-flexy/div[5]/div[1]/div/div[2]/div[5]/div[1]/div[2]/ytd-video-primary-info-renderer/div/div/div[1]/div[1]/ytd-video-view-count-renderer/span[1]').text\n",
    "        channel = self.driver.find_element(By.XPATH, '/html/body/ytd-app/div[1]/ytd-page-manager/ytd-watch-flexy/div[5]/div[1]/div/div[2]/div[5]/div[2]/div[2]/ytd-video-secondary-info-renderer/div/div/ytd-video-owner-renderer/div[1]/ytd-channel-name/div/div/yt-formatted-string/a').text\n",
    "\n",
    "        output['reply'] = str_comments\n",
    "\n",
    "        # 댓글을 먼저 채우고 밑에를 채워야함\n",
    "        output['영상명'] = title\n",
    "        output['채널명'] = channel\n",
    "        output['조회수'] = view\n",
    "        output['URL'] = href\n",
    "        return output\n",
    "    \n",
    "    @staticmethod\n",
    "    def shortCounter(inputDf):\n",
    "        LS = list(inputDf['URL'].values)\n",
    "        count = 0\n",
    "\n",
    "        for val in LS:\n",
    "            if 'short' in val:\n",
    "                count += 1\n",
    "        print('>> 쇼츠 개수: ', count)\n",
    "        # return count\n",
    "\n",
    "    def D2(self, resultOfD1,\n",
    "                fromIdx=0, # 몇번째 for loop부터 돌릴지..\n",
    "                howMuch=5, # 크롤러가 오류가 날 수 있으므로, N등분해서 크롤링해줘야함\n",
    "                manualMode = False, # 수동으로 인덱싱할 때 \n",
    "                ):\n",
    "        output = pd.DataFrame(columns='영상명 채널명 조회수 URL reply'.split())\n",
    "        resultOfD1.reset_index(drop=True, inplace=True)\n",
    "        NextCrawler.shortCounter(resultOfD1) # find shorts video count \n",
    "        window = round(resultOfD1.shape[0]/howMuch)\n",
    "\n",
    "        url = 0 # 에러핸들링용 변수설정임, 아무 의미 없음\n",
    "        try:\n",
    "            for N in range(fromIdx, howMuch):\n",
    "                if N == (howMuch - 1): # last index\n",
    "                    sliceDf = resultOfD1.loc[N*window : , :]\n",
    "                else:\n",
    "                    sliceDf = resultOfD1.loc[N*window : (N+1)*window, :]\n",
    "                \n",
    "                sliceDf.reset_index(drop=True, inplace=True) # 해줘야 아래 코드가 돌아감\n",
    "                \n",
    "                if manualMode == True:\n",
    "                    for idx in tqdm(range(20), desc=f'\\n>> Reply crawling, {resultOfD1.shape[0]}'):\n",
    "                        url = resultOfD1.loc[idx, 'URL']\n",
    "                        if 'short' in url:\n",
    "                            newUrl = url.replace('shorts/', 'watch?v=')\n",
    "                            currentOutput = NextCrawler.earnReply(self, newUrl)\n",
    "                        else:\n",
    "                            currentOutput = NextCrawler.earnReply(self, url)\n",
    "                        print(f'>> {currentOutput.shape[0]} replies collected')\n",
    "                        output = output.append(currentOutput, ignore_index=True)\n",
    "            \n",
    "                else:\n",
    "                    for idx in tqdm(range(sliceDf.shape[0]), desc=f'\\n>> Reply crawling, {sliceDf.shape[0]}'):\n",
    "                        url = sliceDf.loc[idx, 'URL']\n",
    "                        if 'short' in url:\n",
    "                            newUrl = url.replace('shorts/', 'watch?v=')\n",
    "                            currentOutput = NextCrawler.earnReply(self, newUrl)\n",
    "                        else:\n",
    "                            currentOutput = NextCrawler.earnReply(self, url)\n",
    "                        print(f'>> {currentOutput.shape[0]} replies collected')\n",
    "                        output = output.append(currentOutput, ignore_index=True)\n",
    "\n",
    "            \n",
    "            return output\n",
    "        \n",
    "        except:\n",
    "            print('>> 여기 긁다가 멈춤: ', url)\n",
    "            return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nxtCrawler = NextCrawler(USER, driverPATH, chrome_opt)\n",
    "# nxtCrawler.earnReply('https://www.youtube.com/watch?v=tz0iEoi-OSo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> 쇼츠 개수:  22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      ">> Reply crawling, 12:   0%|          | 0/12 [00:00<?, ?it/s]\n",
      ">> Reply crawling, 12:   8%|▊         | 1/12 [00:10<01:56, 10.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> 20 replies collected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      ">> Reply crawling, 12:  17%|█▋        | 2/12 [00:20<01:41, 10.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> 2 replies collected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      ">> Reply crawling, 12:  25%|██▌       | 3/12 [00:30<01:30, 10.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> 6 replies collected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      ">> Reply crawling, 12:  33%|███▎      | 4/12 [00:40<01:20, 10.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> 5 replies collected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      ">> Reply crawling, 12:  42%|████▏     | 5/12 [00:52<01:14, 10.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> 12 replies collected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      ">> Reply crawling, 12:  50%|█████     | 6/12 [01:01<01:01, 10.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> 2 replies collected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      ">> Reply crawling, 12:  58%|█████▊    | 7/12 [01:12<00:52, 10.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> 20 replies collected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      ">> Reply crawling, 12:  67%|██████▋   | 8/12 [01:21<00:40, 10.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> 3 replies collected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      ">> Reply crawling, 12:  75%|███████▌  | 9/12 [01:31<00:29,  9.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> 3 replies collected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      ">> Reply crawling, 12:  83%|████████▎ | 10/12 [01:40<00:19,  9.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> 2 replies collected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      ">> Reply crawling, 12:  92%|█████████▏| 11/12 [01:49<00:09,  9.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> 11 replies collected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      ">> Reply crawling, 12: 100%|██████████| 12/12 [01:59<00:00,  9.56s/it]\n",
      ">> Reply crawling, 12: 100%|██████████| 12/12 [01:59<00:00,  9.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> 17 replies collected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      ">> Reply crawling, 11:   0%|          | 0/11 [00:00<?, ?it/s]\n",
      ">> Reply crawling, 11:   9%|▉         | 1/11 [00:10<01:41, 10.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> 17 replies collected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      ">> Reply crawling, 11:  18%|█▊        | 2/11 [00:19<01:28,  9.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> 14 replies collected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      ">> Reply crawling, 11:  27%|██▋       | 3/11 [00:23<00:55,  6.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> comments are turned off:  https://www.youtube.com/watch?v=2f5o1m9k5Gw\n",
      ">> no reply:  https://www.youtube.com/watch?v=2f5o1m9k5Gw\n",
      ">> 0 replies collected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      ">> Reply crawling, 11:  36%|███▋      | 4/11 [00:32<00:55,  7.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> 20 replies collected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      ">> Reply crawling, 11:  45%|████▌     | 5/11 [00:41<00:50,  8.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> 20 replies collected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      ">> Reply crawling, 11:  55%|█████▍    | 6/11 [00:49<00:40,  8.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> no reply:  https://www.youtube.com/watch?v=aplGrhjNJ1I\n",
      ">> 0 replies collected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      ">> Reply crawling, 11:  64%|██████▎   | 7/11 [00:58<00:34,  8.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> 20 replies collected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      ">> Reply crawling, 11:  73%|███████▎  | 8/11 [01:08<00:26,  8.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> 13 replies collected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      ">> Reply crawling, 11:  82%|████████▏ | 9/11 [01:17<00:18,  9.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> 20 replies collected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      ">> Reply crawling, 11:  91%|█████████ | 10/11 [01:27<00:09,  9.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> 20 replies collected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      ">> Reply crawling, 11: 100%|██████████| 11/11 [01:37<00:00,  9.38s/it]\n",
      ">> Reply crawling, 11: 100%|██████████| 11/11 [01:37<00:00,  8.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> 20 replies collected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## 전체 크롤러\n",
    "# nxtCrawler = NextCrawler(USER, driverPATH, chrome_opt)\n",
    "# D2_RS = nxtCrawler.D2(resultOfD1= D1_RS,\n",
    "#                     # fromIdx=1``\n",
    "#                     )\n",
    "\n",
    "\n",
    "## 22.08.09 쇼츠만\n",
    "ToDrop = []\n",
    "for idx, val in enumerate(D1_RS['URL'].values):\n",
    "    if 'short' not in val:\n",
    "        ToDrop.append(idx)\n",
    "\n",
    "D1_RS_only_short = D1_RS.drop(ToDrop, axis=0)\n",
    "\n",
    "\n",
    "nxtCrawler = NextCrawler(USER, driverPATH, chrome_opt)\n",
    "D2_RS = nxtCrawler.D2(resultOfD1= D1_RS_only_short,\n",
    "                    howMuch = 2,\n",
    "                    # fromIdx=1``\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "D2_RS.to_excel('오직 쇼츠만.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D2_RS = pd.read_excel('crawled(220807)v2.xlsx')\n",
    "\n",
    "class FinalClenaer:\n",
    "    def __init__(self, D1, D2):\n",
    "        self.d1 = D1\n",
    "        self.d2 = D2\n",
    "\n",
    "    \n",
    "    def commentAreTurnedOff(self, inputDF): # 최종 결과를 넣어야함\n",
    "        fullUrl = list(np.unique(self.d1['URL'].values))\n",
    "        assert len(fullUrl) == 100\n",
    "\n",
    "        withoutShort = [url for url in fullUrl if 'short' not in url]\n",
    "        print(len(withoutShort))\n",
    "        crawled = list(np.unique(inputDF['URL'].values))\n",
    "        \n",
    "        shouldBeChecked = []\n",
    "        for url in withoutShort:\n",
    "            if url not in crawled:\n",
    "                shouldBeChecked.append(url)\n",
    "\n",
    "        print('>> should be checked: ', shouldBeChecked)\n",
    "        return shouldBeChecked\n",
    "\n",
    "    def customCleaner(self):\n",
    "        assert self.d1.shape[0] == 100, print('>> ', self.d1.shape[0])\n",
    "        self.d2.reset_index(drop=True, inplace=True)\n",
    "        before = self.d2.shape[0]\n",
    "        filledIdx = [idx for idx, title in enumerate(self.d2['영상명'].values) if type(title) == str]\n",
    "        forMerge = self.d2.loc[filledIdx, :]\n",
    "        print('>> for merge: ', forMerge.shape[0])\n",
    "\n",
    "        notFilled = self.d2.drop(filledIdx, axis=0)\n",
    "        print('>> to be filled: ', notFilled.shape[0])\n",
    "        assert notFilled.shape[0] + forMerge.shape[0] == before # 이걸 꼭 해줘야함\n",
    "        \n",
    "        crawled = list(np.unique(notFilled['URL'].values))\n",
    "        print('>> to be filled video: ', len(crawled))\n",
    "        for idx, (url, title, view) in enumerate(zip(self.d1['URL'], self.d1['영상명'], self.d1['조회수'])): # 전체 영상에 대해서 루프돌려서\n",
    "            if url in crawled:\n",
    "                # if idx < 15:\n",
    "                #     whereIsit = notFilled[notFilled['URL'] == url].index\n",
    "                #     notFilled.loc[whereIsit, '영상명'] = title\n",
    "                #     notFilled.loc[whereIsit, '조회수'] = view\n",
    "                    # print(notFilled.loc[whereIsit, :])\n",
    "                whereIsit = notFilled[notFilled['URL'] == url].index\n",
    "                notFilled.loc[whereIsit, '영상명'] = title\n",
    "                notFilled.loc[whereIsit, '조회수'] = view\n",
    "\n",
    "        output = pd.concat([forMerge, notFilled], axis=0)\n",
    "        return output\n",
    "\n",
    "cleaner = FinalClenaer(D1= D1_RS, D2= D2_RS)\n",
    "finalRS = cleaner.customCleaner() # 데이터 정리\n",
    "NextCrawler.shortCounter(D1_RS) # 쇼츠 개수 파악\n",
    "beChecked = cleaner.commentAreTurnedOff(inputDF = finalRS) # 댓글 없는 영상 재검토\n",
    "\n",
    "\n",
    "with pd.ExcelWriter('finalRS(220808).xlsx') as writer:\n",
    "    D1_RS.to_excel(writer, sheet_name='선별된 영상 100개')\n",
    "    finalRS.to_excel(writer, sheet_name='댓글 크롤링 결과')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 08.09 쇼츠랑 합치기\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('SNURO')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5702d5a1f35543d6c34eea9cfe7c421721e3098aad62c19242cc5f6d6a95c445"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
